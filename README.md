# WaveWing-Capstone-
# ğŸ›°ï¸ WaveWing - wave your hands to control wings

**WaveWing** is an interactive system that allows users to control a drone using **hand gestures**.  
It bridges **MediaPipe-based gesture recognition (Windows)** and **DroneKit-based drone control (Ubuntu)** via a **WebSocket communication layer**.

---

## ğŸ¯ Project Overview

**Workflow:**
```
Camera â†’ Gesture Detection (MediaPipe + CNN)
       â†’ WebSocket Client (Windows)
       â†’ Command Queue & Ack Buffer
       â†’ WebSocket Server (Ubuntu)
       â†’ DroneKit (MAVLink API)
       â†’ ArduPilot SITL â†’ Gazebo Simulation
```

Our system recognizes real-time gestures through a webcam and translates them into drone commands like  
**TAKEOFF**, **LAND**, **FORWARD**, **LEFT**, **RIGHT**, **UP**, **DOWN**, and **YAW ROTATION**.

---

## âš™ï¸ Components

### ğŸ– Gesture Detection (Windows Frontend)
- Built with **MediaPipe** and **OpenCV** for 3D hand-landmark tracking.  
- A **5-layer CNN classifier** trained on our custom dataset interprets keypoint coordinates into labeled gestures.  
- Real-time FPS optimized at ~30 fps.  
- Outputs a JSON payload to WebSocket:
  ```json
  {"gesture": "TAKEOFF"}
  ```

### ğŸŒ WebSocket Bridge
- Uses Pythonâ€™s `asyncio` + `websockets` for low-latency data transmission.  
- Handles acknowledgments, buffering, and command queuing to ensure smooth control.  
- Enables seamless **Windows â†’ Ubuntu** communication.

### ğŸš Drone Control (Ubuntu Backend)
- Powered by **DroneKit Python** connected to **ArduPilot SITL** or a physical flight controller.  
- Commands interpreted through **MAVLink protocol**.  
- Simulated in **Gazebo 7** using **Iris Gimbal** model and custom runway world.  
- Each received gesture triggers precise drone actions with velocity-vector control.

---

## ğŸ§  Model Training

1. **Data Collection:**  
   - Capture hand-landmark coordinates from MediaPipe.  
   - Normalize and label data (e.g., `Takeoff`, `Land`, `Hover`).

2. **Training:**  
   - Run `keypoint_training.ipynb` to train the CNN classifier.  
   - Architecture: Dense-ReLU layers â†’ Softmax output.  
   - Framework: TensorFlow 2.x.

3. **Deployment:**  
   - Export model as `.tflite` for lightweight inference.  
   - Integrated with `gesture_app.py` for real-time prediction.

---

## ğŸ§© System Requirements

| Component | Version |
|------------|----------|
| Python | â‰¥ 3.8 |
| MediaPipe | â‰¥ 0.10 |
| OpenCV | â‰¥ 4.5 |
| TensorFlow | â‰¥ 2.8 |
| DroneKit | Latest |
| ArduPilot SITL | 4.3 + |
| Gazebo | â‰¥ 7 |

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Launch Gesture Detection (Windows)
```bash
python gesture_app.py
```
- Opens webcam stream and starts sending gestures via WebSocket.

### 2ï¸âƒ£ Start WebSocket Server (Ubuntu)
```bash
python server_ws.py
```
- Connects to the drone through DroneKit (`127.0.0.1:14550`).  
- Receives gesture commands and converts them into movement instructions.

### 3ï¸âƒ£ Run Drone Simulation
```bash
sim_vehicle.py -v ArduCopter -f gazebo-iris --console --map
```
- Launches ArduPilot SITL in Gazebo environment.

---

## ğŸ”¢ Repository Structure
```
â”‚  gesture_app.py
â”‚  server_ws.py
â”‚  keypoint_training.ipynb
â”‚
â”œâ”€ model/
â”‚   â”œâ”€ keypoint_classifier/
â”‚   â”‚   â”œâ”€ keypoint_classifier.tflite
â”‚   â”‚   â”œâ”€ keypoint_classifier_label.csv
â”‚   â”‚   â””â”€ keypoint.csv
â”‚   â””â”€ gesture_logs/
â”‚       â””â”€ collected_datasets/
â”‚
â””â”€ utils/
    â”œâ”€ cvfpscalc.py
    â””â”€ command_buffer.py
```

---

## System Architecture
<img width="774" height="434" alt="image" src="https://github.com/user-attachments/assets/57d9c325-d2bd-4597-8222-9221de2a97c2" />

<img width="997" height="586" alt="image" src="https://github.com/user-attachments/assets/a9ef5d09-c536-4862-838b-d551d905e81c" />
<img width="646" height="585" alt="image" src="https://github.com/user-attachments/assets/b5b0785f-8645-4079-93cb-3dfb9918c1da" />



---

## ğŸ“Š Tech Stack
- **Python** (OpenCV, TensorFlow, DroneKit)
- **MediaPipe** for hand landmark detection
- **WebSocket (asyncio + websockets)** for real-time messaging
- **Gazebo 7 + ArduPilot SITL** for simulation
- **MAVLink Protocol** for flight communication


---

## ğŸ”– Credits & Acknowledgements
This project was **inspired by the open-source work** [*hand-gesture-recognition-using-mediapipe*](https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe) by **Kazuhito Takahashi**.  
We extended and adapted that foundational gesture model into a **complete drone control interface** integrating WebSocket communication, DroneKit API, and ArduPilot Gazebo simulation.

---

## ğŸ“„ License
This project is released under the **Apache License 2.0**.
